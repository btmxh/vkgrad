#version 450
#extension GL_KHR_vulkan_memory_model: enable
#extension GL_EXT_control_flow_attributes : enable
#extension GL_EXT_integer_dot_product: enable

#ifndef BLOCK_M
#define BLOCK_M 64
#endif

#ifndef BLOCK_N
#define BLOCK_N 64
#endif

#ifndef BLOCK_K
#define BLOCK_K 8
#endif

#ifndef THREAD_M
#define THREAD_M 8
#endif

layout(local_size_x = BLOCK_M * BLOCK_N / THREAD_M) in;

layout(std430, binding = 0) readonly buffer ABuffer {
    float A[];
};

layout(std430, binding = 1) readonly buffer BBuffer {
    float B[];
};

layout(std430, binding = 2) buffer CBuffer {
    float C[];
};

layout(push_constant) uniform PushConsts {
    int M; // rows of A and C
    int N; // cols of B and C
    int K; // cols of A, rows of B
    ivec2 stride_A;
    ivec2 stride_B;
    ivec2 stride_C;
    float alpha;
    float beta;
};

shared float shared_A[BLOCK_M * BLOCK_K];
shared float shared_B[BLOCK_K * BLOCK_N];

#define matrix_get(matrix, pos, stride, size) \
    ((pos.x) < (size).x && (pos.y) < (size).y ? matrix[dotEXT((pos), (stride))] : 0.0)
#define A_get(pos) matrix_get(A, pos, stride_A, ivec2(M, K))
#define B_get(pos) matrix_get(B, pos, stride_B, ivec2(K, N))
#define C_get(pos) matrix_get(C, pos, stride_C, ivec2(M, N))
#define C_set(pos, value) if((pos.x) < M && (pos.y) < N) C[dotEXT((pos), stride_C)] = (value)

// x is row, y is col
void main() {
    // these are coordinates for reading/writing C
    const uint local_row = gl_LocalInvocationID.x / BLOCK_N;
    const uint local_col = gl_LocalInvocationID.x % BLOCK_N;
    const uvec2 local_pos = uvec2(local_row, local_col);

    // these are coordinates for reading A and B to shared memory
    const uint local_row_A = gl_LocalInvocationID.x / BLOCK_K;
    const uint local_col_A = gl_LocalInvocationID.x % BLOCK_K;
    const uint local_row_B = gl_LocalInvocationID.x / BLOCK_N;
    const uint local_col_B = gl_LocalInvocationID.x % BLOCK_N;
    const uvec2 local_pos_A = uvec2(local_row_A, local_col_A);
    const uvec2 local_pos_B = uvec2(local_row_B, local_col_B);

    // transpose trick idk
    const uvec2 block_pos_C = gl_WorkGroupID.xy * ivec2(BLOCK_M, BLOCK_N);
    const uvec2 thread_pos_C = block_pos_C + local_pos * uvec2(THREAD_M, 1);

    float dot_product_total[THREAD_M];
    for (int i = 0; i < THREAD_M; ++i) {
        dot_product_total[i] = 0.0;
    }

    int k = 0;
    for (int k = 0; k < K; k += BLOCK_K) {
        // Load tiles of A and B into shared_A and shared_B
        const uvec2 block_pos_A = uvec2(block_pos_C.x, k);
        const uvec2 block_pos_B = uvec2(k, block_pos_C.y);
        const uvec2 thread_pos_A = block_pos_A + local_pos_A;
        const uvec2 thread_pos_B = block_pos_B + local_pos_B;

        // Note that since number of threads != number of elements in shared_A
        // and shared_B, a thread might have to load multiple elements or none
        // at all
        #define CEIL_DIV(M, N) (((M) + (N)-1) / (N))
        #define NUM_TO_LOAD_A CEIL_DIV(shared_A.length(), gl_WorkGroupSize.x)
        #define NUM_TO_LOAD_B CEIL_DIV(shared_B.length(), gl_WorkGroupSize.x)

        for (int i = 0; i < NUM_TO_LOAD_A; ++i) {
            const uint thread_pos_shared_A = (local_row_A + i * gl_WorkGroupSize.x / BLOCK_K) * BLOCK_K + local_col_A;
            const uvec2 thread_pos_A_i = thread_pos_A + uvec2(i * gl_WorkGroupSize.x / BLOCK_K, 0);
            if (thread_pos_shared_A < BLOCK_M * BLOCK_K) {
                shared_A[thread_pos_shared_A] = A_get(ivec2(thread_pos_A_i));
            }
        }
        for (int i = 0; i < NUM_TO_LOAD_B; ++i) {
            const uint thread_pos_shared_B = (local_row_B + i * gl_WorkGroupSize.x / BLOCK_N) * BLOCK_N + local_col_B;
            const uvec2 thread_pos_B_i = thread_pos_B + uvec2(i * gl_WorkGroupSize.x / BLOCK_N, 0);
            if (thread_pos_shared_B < BLOCK_K * BLOCK_N) {
                shared_B[thread_pos_shared_B] = B_get(ivec2(thread_pos_B_i));
            }
        }

        barrier();

        // Compute partial dot product for this tile, j is the dot product index
        for (int j = 0; j < BLOCK_K; ++j) {
            // we use the same b for all m in THREAD_M
            float b = shared_B[j * BLOCK_N + local_col];
            for (int m = 0; m < THREAD_M; ++m) {
                float a = shared_A[(local_row * THREAD_M + m) * BLOCK_K + j];
                dot_product_total[m] += a * b;
            }
        }

        barrier();
    }

    for (int m = 0; m < THREAD_M; ++m) {
        uvec2 pos_C = thread_pos_C + uvec2(m, 0);
        float final_value = alpha * dot_product_total[m] + beta * C_get(ivec2(pos_C));
        C_set(pos_C, final_value);
    }
}
